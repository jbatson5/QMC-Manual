

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Running QMCPACK &mdash; QMC Manual 0.0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Units used in QMCPACK" href="units.html" />
    <link rel="prev" title="Obtaining, installing, and validating QMCPACK" href="installation.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> QMC Manual
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="features.html">Features of QMCPACK</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Obtaining, installing, and validating QMCPACK</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Running QMCPACK</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#command-line-options">Command line options</a></li>
<li class="toctree-l2"><a class="reference internal" href="#input-files">Input files</a></li>
<li class="toctree-l2"><a class="reference internal" href="#output-files">Output files</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-in-parallel-with-mpi">Running in parallel with MPI</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-openmp-threads">Using OpenMP threads</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#nested-openmp-threads">Nested OpenMP threads</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performance-considerations">Performance considerations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#memory-considerations">Memory considerations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#running-on-gpu-machines">Running on GPU machines</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#gpu-performance">Performance considerations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">Memory considerations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="units.html">Units used in QMCPACK</a></li>
<li class="toctree-l1"><a class="reference internal" href="input_overview.html">Input file overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="simulationcell.html">Specifying the system to be simulated</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_wavefunction.html">Trial wavefunction specificaion</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">QMC Manual</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Running QMCPACK</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/running.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="running-qmcpack">
<span id="running"></span><h1>Running QMCPACK<a class="headerlink" href="#running-qmcpack" title="Permalink to this headline">¶</a></h1>
<p>QMCPACK requires at least one xml input file, and is invoked via:</p>
<p><code class="docutils literal notranslate"><span class="pre">qmcpack</span> <span class="pre">[command</span> <span class="pre">line</span> <span class="pre">options]</span> <span class="pre">&lt;XML</span> <span class="pre">input</span> <span class="pre">file(s)&gt;</span></code></p>
<div class="section" id="command-line-options">
<h2>Command line options<a class="headerlink" href="#command-line-options" title="Permalink to this headline">¶</a></h2>
<p>QMCPACK offers several command line options that affect how calculations
are performed. If the flag is absent, then the corresponding
option is disabled:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--dryrun</span></code> Validate the input file without performing the simulation. This is a good way to ensure that QMCPACK will do what you think it will.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--enable-timers=none|coarse|medium|fine</span></code> Control the timer granularity when the build option <code class="docutils literal notranslate"><span class="pre">ENABLE_TIMERS</span></code> is enabled.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">help</span></code> Print version information as well as a list of optional
command-line arguments.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">noprint</span></code> Do not print extra information on Jastrow or pseudopotential.
If this flag is not present, QMCPACK will create several <code class="docutils literal notranslate"><span class="pre">.dat</span></code> files
that contain information about pseudopotentials (one file per PP) and Jastrow
factors (one per Jastrow factor). These file might be useful for visual inspection
of the Jastrow, for example.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--verbosity=low|high|debug</span></code> Control the output verbosity. The default low verbosity is concise and, for example, does not include all electron or atomic positions for large systems to reduce output size. Use “high” to see this information and more details of initialization, allocations, QMC method settings, etc.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">version</span></code> Print version information and optional arguments. Same as <code class="docutils literal notranslate"><span class="pre">help</span></code>.</p></li>
</ul>
</div>
<div class="section" id="input-files">
<span id="inputs"></span><h2>Input files<a class="headerlink" href="#input-files" title="Permalink to this headline">¶</a></h2>
<p>The input is one or more XML file(s), documented in <span class="xref std std-ref">input_overview</span>.</p>
</div>
<div class="section" id="output-files">
<h2>Output files<a class="headerlink" href="#output-files" title="Permalink to this headline">¶</a></h2>
<p>QMCPACK generates multiple files documented in <span class="xref std std-ref">output_overview</span>.</p>
</div>
<div class="section" id="running-in-parallel-with-mpi">
<span id="parallelrunning"></span><h2>Running in parallel with MPI<a class="headerlink" href="#running-in-parallel-with-mpi" title="Permalink to this headline">¶</a></h2>
<p>QMCPACK is fully parallelized with MPI. When performing an ensemble job, all
the MPI ranks are first equally divided into groups that perform individual
QMC calculations. Within one calculation, all the walkers are fully distributed
across all the MPI ranks in the group. Since MPI requires distributed memory,
there must be at least one MPI per node. To maximize the efficiency, more facts
should be taken into account. When using MPI+threads on compute nodes with more
than one NUMA domain (e.g., AMD Interlagos CPU on Titan or a node with multiple
CPU sockets), it is recommended to place as many MPI ranks as the number of
NUMA domains if the memory is sufficient (e.g., one MPI task per socket). On clusters with more than one
GPU per node (NVIDIA Tesla K80), it is necessary to use the same number of MPI
ranks as the number of GPUs per node to let each MPI rank take one GPU.</p>
</div>
<div class="section" id="using-openmp-threads">
<span id="openmprunning"></span><h2>Using OpenMP threads<a class="headerlink" href="#using-openmp-threads" title="Permalink to this headline">¶</a></h2>
<p>Modern processors integrate multiple identical cores even with
hardware threads on a single die to increase the total performance and
maintain a reasonable power draw. QMCPACK takes advantage of this
compute capability by using threads and the OpenMP programming model
as well as threaded linear algebra libraries. By default, QMCPACK is
always built with OpenMP enabled. When launching calculations, users
should instruct QMCPACK to create the right number of threads per MPI
rank by specifying environment variable OMP_NUM_THREADS. Assuming
one MPI rank per socket, the number of threads should typically be the
number of cores on that socket. Even in the GPU-accelerated version,
using threads significantly reduces the time spent on the calculations
performed by the CPU.</p>
<div class="section" id="nested-openmp-threads">
<h3>Nested OpenMP threads<a class="headerlink" href="#nested-openmp-threads" title="Permalink to this headline">¶</a></h3>
<p>Nested threading is an advanced feature requiring experienced users to finely tune runtime parameters to reach the best performance.</p>
<p>For small-to-medium problem sizes, using one thread per walker or for multiple walkers is most efficient. This is the default in QMCPACK and achieves the shortest time to solution.</p>
<p>For large problems of at least 1,000 electrons, use of nested OpenMP threading can be enabled to reduce the time to solution further, although at some loss of efficiency. In this scheme multiple threads are used in the computations of each walker. This capability is implemented for some of the key computational kernels: the 3D spline orbital evaluation, certain portions of the distance tables, and implicitly the BLAS calls in the determinant update. Use of the batched nonlocal pseudopotential evaluation is also recommended.</p>
<p>Nested threading is enabled by setting <code class="docutils literal notranslate"><span class="pre">OMP\_NUM\_THREADS=AA,BB</span></code>, <code class="docutils literal notranslate"><span class="pre">OMP\_MAX\_ACTIVE\_LEVELS=2</span></code> and <code class="docutils literal notranslate"><span class="pre">OMP\_NESTED=TRUE</span></code> where the additional <code class="docutils literal notranslate"><span class="pre">BB</span></code> is the number of second-level threads.  Choosing the thread affinity is critical to the performance.
QMCPACK provides a tool qmc-check-affinity (source file src/QMCTools/check-affinity.cpp for details), which might help users investigate the affinity. Knowledge of how the operating system logical CPU cores (/prco/cpuinfo) are bound to the hardware is also needed.</p>
<p>For example, on Blue Gene/Q with a Clang compiler, the best way to fully use the 16 cores each with 4 hardware threads is</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">OMP_NESTED</span><span class="o">=</span><span class="n">TRUE</span>
<span class="n">OMP_NUM_THREADS</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span><span class="mi">4</span>
<span class="n">MAX_ACTIVE_LEVELS</span><span class="o">=</span><span class="mi">2</span>
<span class="n">OMP_PLACES</span><span class="o">=</span><span class="n">threads</span>
<span class="n">OMP_PROC_BIND</span><span class="o">=</span><span class="n">spread</span><span class="p">,</span><span class="n">close</span>
</pre></div>
</div>
<p>On Intel Xeon Phi KNL with an Intel compiler, to use 64 cores without using hardware threads:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">OMP_NESTED</span><span class="o">=</span><span class="n">TRUE</span>
<span class="n">OMP_WAIT_POLICY</span><span class="o">=</span><span class="n">ACTIVE</span>
<span class="n">OMP_NUM_THREADS</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span><span class="mi">4</span>
<span class="n">MAX_ACTIVE_LEVELS</span><span class="o">=</span><span class="mi">2</span>
<span class="n">OMP_PLACES</span><span class="o">=</span><span class="n">cores</span>
<span class="n">OMP_PROC_BIND</span><span class="o">=</span><span class="n">spread</span><span class="p">,</span><span class="n">close</span>
<span class="n">KMP_HOT_TEAMS_MODE</span><span class="o">=</span><span class="mi">1</span>
<span class="n">KMP_HOT_TEAMS_MAX_LEVEL</span><span class="o">=</span><span class="mi">2</span>
</pre></div>
</div>
<p>Most multithreaded BLAS/LAPACK libraries do not spawn threads by default
when being called from an OpenMP parallel region. See the explanation in <a class="reference internal" href="installation.html#threadedlibrary"><span class="std std-ref">Serial or multithreaded library</span></a>.
This results in the use of only a single thread in each second-level thread team for BLAS/LAPACK operations.
Some vendor libraries like MKL support using multiple threads when being called from an OpenMP parallel region.
One way to enable this feature is using environment variables to override the default behavior.
However, this forces all the calls to the library to use the same number of threads.
As a result, small function calls are penalized with heavy overhead and heavy function calls are slow for not being able to use more threads.
Instead, QMCPACK uses the library APIs to turn on nested threading only at selected performance critical calls.
In the case of using a serial library, QMCPACK implements nested threading to distribute the workload wherever necessary.
Users do not need to control the threading behavior of the library.</p>
</div>
<div class="section" id="performance-considerations">
<span id="cpu-performance"></span><h3>Performance considerations<a class="headerlink" href="#performance-considerations" title="Permalink to this headline">¶</a></h3>
<p>As walkers are the basic units of workload in QMC algorithms, they are loosely coupled and distributed across all the threads. For this reason, the best strategy to run QMCPACK efficiently is to feed enough walkers to the available threads.</p>
<p>In a VMC calculation, the code automatically raises the actual number of walkers per MPI rank to the number of available threads
if the user-specified number of walkers is smaller, see “walkers/mpi=XXX” in the VMC output.</p>
<p>In DMC, for typical small to mid-sized calculations choose the total number of walkers to be a significant multiple of the total number of
threads (MPI tasks * threads per task). This will ensure a good load balance. e.g., for a calculation on a few nodes with a total
512 threads, using 5120 walkers may keep the load imbalance around 10%. For the very largest calculations, the target number of
walkers should be chosen to be slightly smaller than a multiple of the total number of available threads across all the MPI ranks.
This will reduce occurrences worse-case load imbalance e.g. where one thread has two walkers while all the others have one.</p>
<p>To achieve better performance, a mixed-precision version (experimental) has been developed in the CPU code. The mixed-precision
CPU code uses a mixed of single precision (SP) and double precision (DP) operations, while the default code use DP exclusively.
This mixed precision version is more aggressive than the GPU CUDA version in using single precision (SP) operations. The Current implementation uses SP on most
calculations, except for matrix inversions and reductions where double precision is required to retain high accuracy. All the
constant spline data in wavefunction, pseudopotentials, and Coulomb potentials are initialized in double precision and later
stored in single precision. The mixed-precision code is as accurate as the double-precision code up to a certain system size, and
may have double the throughput.
Cross checking and verification of accuracy is always required but is particularly important above approximately 1,500 electrons.</p>
</div>
<div class="section" id="memory-considerations">
<h3>Memory considerations<a class="headerlink" href="#memory-considerations" title="Permalink to this headline">¶</a></h3>
<p>When using threads, some memory objects are shared by all the threads. Usually these memory objects are read only when the walkers are evolving, for instance the ionic distance table and wavefunction coefficients.
If a wavefunction is represented by B-splines, the whole table is shared by all the threads. It usually takes a large chunk of memory when a large primitive cell was used in the simulation. Its actual size is reported as “MEMORY increase XXX MB BsplineSetReader” in the output file.
See details about how to reduce it in <a class="reference internal" href="intro_wavefunction.html#spo-spline"><span class="std std-ref">Spline basis sets</span></a>.</p>
<p>The other memory objects that are distinct for each walker during random walks need to be
associated with individual walkers and cannot be shared. This part of memory grows linearly as the number of walkers per MPI rank. Those objects include wavefunction values (Slater determinants) at given electronic configurations and electron-related distance tables (electron-electron distance table). Those matrices dominate the <span class="math notranslate nohighlight">\(N^2\)</span> scaling of the memory usage per walker.</p>
</div>
</div>
<div class="section" id="running-on-gpu-machines">
<span id="gpurunning"></span><h2>Running on GPU machines<a class="headerlink" href="#running-on-gpu-machines" title="Permalink to this headline">¶</a></h2>
<p>The GPU version for the NVIDIA CUDA platform is fully incorporated into
the main source code. Commonly used functionalities for
solid-state and molecular systems using B-spline single-particle
orbitals are supported. Use of Gaussian basis sets, three-body
Jastrow functions, and many observables are not yet supported. A detailed description of the GPU
implementation can be found in <a class="bibtex reference internal" href="#eslerkimceperleyshulenburger2012" id="id1">[EKCS12]</a>.</p>
<p>The current GPU implementation assumes one MPI process per GPU. To use
nodes with multiple GPUs, use multiple MPI processes per node.
Vectorization is achieved over walkers, that is, all walkers are
propagated in parallel. In each GPU kernel, loops over electrons,
atomic cores, or orbitals are further vectorized to exploit an
additional level of parallelism and to allow coalesced memory access.</p>
<div class="section" id="gpu-performance">
<span id="id2"></span><h3>Performance considerations<a class="headerlink" href="#gpu-performance" title="Permalink to this headline">¶</a></h3>
<p>To run with high performance on GPUs it is crucial to perform some
benchmarking runs: the optimum configuration is system size, walker
count, and GPU model dependent. The GPU implementation vectorizes
operations over multiple walkers, so generally the more walkers that
are placed on a GPU, the higher the performance that will be
obtained. Performance also increases with electron count, up until the
memory on the GPU is exhausted. A good strategy is to perform a short
series of VMC runs with walker count increasing in multiples of
two. For systems with 100s of electrons, typically 128–256 walkers per
GPU use a sufficient number of GPU threads to operate the GPU
efficiently and to hide memory-access latency. For smaller systems,
thousands of walkers might be required. For QMC algorithms where the number of
walkers is fixed such as VMC, choosing a walker count the is a multiple of the
number of streaming multiprocessors can be most efficient. For
variable population DMC runs, this exact match is not possible.</p>
<p>To achieve better performance, the current GPU implementation uses
single-precision operations for most of the calculations. Double
precision is used in matrix inversions and the Coulomb interaction to
retain high accuracy. The mixed-precision GPU code is as accurate as
the double-precision CPU code up to a certain system size. Cross
checking and verification of accuracy are encouraged for systems with
more than approximately 1,500 electrons. For typical calculations on
smaller electron counts, the statistical error bars are much larger
then the error introduced by mixed precision.</p>
</div>
<div class="section" id="id3">
<h3>Memory considerations<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>In the GPU implementation, each walker has a buffer in the GPU’s
global memory to store temporary data associated with the
wavefunctions. Therefore, the amount of memory available on a GPU
limits the number of walkers and eventually the system size that it
can process. Additionally, for calculations using B-splines, this data
is stored on the GPU in a shared read-only buffer. Often the size of the
B-spline data limits the calculations that can be run on the GPU.</p>
<p>If the GPU memory is exhausted, first try reducing the number of walkers per GPU.
Coarsening the grids of the B-splines representation (by decreasing
the value of the mesh factor in the input file) can also lower the memory
usage, at the expense (risk) of obtaining inaccurate results. Proceed
with caution if this option has to be considered.  It is also possible
to distribute the B-spline coefficients table between the host and GPU
memory, see option Spline_Size_Limit_MB in
<a class="reference internal" href="intro_wavefunction.html#spo-spline"><span class="std std-ref">Spline basis sets</span></a>.</p>
<p id="bibtex-bibliography-running-0"><dl class="citation">
<dt class="bibtex label" id="eslerkimceperleyshulenburger2012"><span class="brackets"><a class="fn-backref" href="#id1">EKCS12</a></span></dt>
<dd><p>Kenneth P. Esler, Jeongnim Kim, David M. Ceperley, and Luke Shulenburger. Accelerating quantum monte carlo simulations of real materials on gpu clusters. <em>Computing in Science and Engineering</em>, 14(1):40–51, 2012. <a class="reference external" href="https://doi.org/http://doi.ieeecomputersociety.org/10.1109/MCSE.2010.122">doi:http://doi.ieeecomputersociety.org/10.1109/MCSE.2010.122</a>.</p>
</dd>
</dl>
</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="units.html" class="btn btn-neutral float-right" title="Units used in QMCPACK" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="installation.html" class="btn btn-neutral float-left" title="Obtaining, installing, and validating QMCPACK" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, John Batson

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>